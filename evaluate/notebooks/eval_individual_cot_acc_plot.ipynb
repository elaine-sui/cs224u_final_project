{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "sys.path.append('../')\n",
    "from evaluate.eval_utils import get_simple_eval_metrics\n",
    "\n",
    "OUTPUT_ROOT = \"/sailhome/esui/cs224u_final_project/prontoqa_output/fictional/aggregated\"\n",
    "\n",
    "def get_metrics_dict_for_prompt_type(prompt_type, k, restrict_type='none'):\n",
    "\n",
    "    dir = os.path.join(prompt_type, \"summary\")\n",
    "    filename = 'merge_answer_hard_merge_cot_none_path_select_heaviest' + f\"_{k}hop.pkl\"\n",
    "    path = os.path.join(OUTPUT_ROOT, dir, filename)\n",
    "\n",
    "    metrics_dict = get_simple_eval_metrics(path, restrict_type=restrict_type)\n",
    "    return metrics_dict\n",
    "\n",
    "def get_data_for_all_metrics(metrics_dict, prompt_types):\n",
    "    data = []\n",
    "    for metric in metrics_dict[prompt_types[0]].keys():\n",
    "        data_metric = [metric]\n",
    "        for prompt_type in prompt_types:\n",
    "            data_metric.append(metrics_dict[prompt_type][metric])\n",
    "        data.append(data_metric)\n",
    "    return data\n",
    "\n",
    "def plot_metric(metrics_df_no_path_selection, metrics_df_w_path_selection, metric, title, save_file):\n",
    "    \n",
    "    # TODO: merge metrics_dfs so that columns are With Path Selection and Without Path Selection\n",
    "\n",
    "    metrics_df = metrics_df[metrics_df['metric'] == metric]\n",
    "    metrics_df = metrics_df.drop(columns=['metric']).T\n",
    "\n",
    "    # Rename the metrics\n",
    "    mapping = {\n",
    "        'avg_label_acc': \"Average Label Accuracy\",\n",
    "        \"avg_cot_acc\": \"Average CoT Accuracy\",\n",
    "        \"avg_cot_precision\": \"Average CoT Precision\",\n",
    "        \"avg_cot_recall\": \"Average CoT Recall\",\n",
    "        \"avg_cot_f1\": \"Average CoT F1\"\n",
    "    }\n",
    "\n",
    "    metric = mapping[metric]\n",
    "\n",
    "    metrics_df = metrics_df.rename(columns={0:metric})\n",
    "    metrics_df = metrics_df.reset_index().rename(columns={'index':'Prompting Strategy'})\n",
    "\n",
    "    # Rename the aggregation types\n",
    "    mapping = {\n",
    "        'single_baseline':'Baseline', \n",
    "        'single_forward': \"Forward Chaining\",\n",
    "        'single_backward': \"Backward Chaining\",\n",
    "        'single_baseline_neg': \"Baseline Negate All Queries\",\n",
    "        'single_forward_neg': \"Forward Negate All Queries\",\n",
    "        'single_backward_neg': \"Backward Negate All Queries\",\n",
    "        'double_baseline_negation': \"Baseline Negate 'not' Queries\",\n",
    "        'double_forward_negation': \"Forward Negate 'not' Queries\",\n",
    "        'double_backward_negation': \"Backward Negate 'not' Queries\",\n",
    "    }\n",
    "    metrics_df['Prompting Strategy'] = metrics_df['Prompting Strategy'].map(mapping)\n",
    "\n",
    "    print(metrics_df)\n",
    "    kind='barh'\n",
    "    ax = metrics_df.plot(\n",
    "        x='Prompting Strategy',\n",
    "        xlabel=metric,\n",
    "        kind=kind,\n",
    "        stacked=False,\n",
    "        title=title,\n",
    "        legend=False,\n",
    "    )\n",
    "\n",
    "    for container in ax.containers:\n",
    "        ax.bar_label(container, fmt='%.2f')\n",
    "    \n",
    "    plt.savefig(save_file, bbox_inches='tight')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "PROMPT_TYPES = [\"single_baseline\", \"single_baseline_neg\", \"double_baseline_negation\", \"single_forward\", \"single_forward_neg\", \"double_forward_negation\", \"single_backward\", \"single_backward_neg\", \"double_backward_negation\"]\n",
    "PROMPT_TYPES = list(reversed(PROMPT_TYPES))\n",
    "columns = [\"metric\"] + PROMPT_TYPES\n",
    "metrics_dict = {}\n",
    "\n",
    "for metric in ['avg_label_acc', 'avg_cot_acc']:\n",
    "    for restrict_type in ['none', 'not_only', 'no_not_only']:\n",
    "        if restrict_type == 'none':\n",
    "            dataset_type = 'all'\n",
    "        else:\n",
    "            dataset_type = restrict_type\n",
    "\n",
    "        for k in [1, 3, 5]:\n",
    "            for prompt_type in PROMPT_TYPES:\n",
    "                metrics_dict[prompt_type] = get_metrics_dict_for_prompt_type(prompt_type, k, restrict_type)\n",
    "\n",
    "            data = get_data_for_all_metrics(metrics_dict, PROMPT_TYPES)\n",
    "\n",
    "            metrics_df = pd.DataFrame(\n",
    "                        columns=columns,\n",
    "                        data=data,\n",
    "            )\n",
    "\n",
    "            # print(metrics_df)\n",
    "\n",
    "            # metrics_df.plot(x='metric',\n",
    "            #     kind='barh',\n",
    "            #     stacked=False,\n",
    "            #     title=\"individual eval no path selection\"\n",
    "            # )\n",
    "\n",
    "            # save to csv\n",
    "            dir = '/sailhome/esui/cs224u_final_project/evaluate/results/single_with_path_selection'\n",
    "            os.makedirs(dir, exist_ok=True)\n",
    "            metrics_df.T.to_csv(os.path.join(dir, f'{dataset_type}_path_selection_heaviest_{k}hop.csv'))\n",
    "\n",
    "            dir = os.path.join(dir, \"plots\")\n",
    "            os.makedirs(dir, exist_ok=True)\n",
    "            plot_metric(metrics_df, metric, title=None, save_file=os.path.join(dir, f'{metric}_{dataset_type}_path_selection_heaviest_{k}hop.pdf'))"
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}